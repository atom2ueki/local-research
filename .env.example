# Local Research Model Configuration
# Configure different models for different purposes using environment variables
# Format: VARIABLE_NAME=provider:model or provider://host:port/model

# ===== STANDARD CLOUD PROVIDERS =====

# OpenAI Models
# REPORT_MODEL=openai:gpt-4o
# RESEARCH_MODEL=openai:gpt-4o
# SUMMARIZATION_MODEL=openai:gpt-4o-mini
# COMPRESS_MODEL=openai:gpt-4o
# SUPERVISOR_MODEL=openai:gpt-4o
# SCOPE_MODEL=openai:gpt-4o

# Anthropic Models
# REPORT_MODEL=anthropic:claude-3-5-sonnet-20241022
# RESEARCH_MODEL=anthropic:claude-3-5-sonnet-20241022
# SUMMARIZATION_MODEL=anthropic:claude-3-5-haiku-20241022
# COMPRESS_MODEL=anthropic:claude-3-5-sonnet-20241022
# SUPERVISOR_MODEL=anthropic:claude-3-5-sonnet-20241022
# SCOPE_MODEL=anthropic:claude-3-5-sonnet-20241022

# Google Generative AI Models
# REPORT_MODEL=google_genai:gemini-2.0-flash-exp
# RESEARCH_MODEL=google_genai:gemini-2.0-flash-exp
# SUMMARIZATION_MODEL=google_genai:gemini-2.0-flash-exp
# COMPRESS_MODEL=google_genai:gemini-2.0-flash-exp
# SUPERVISOR_MODEL=google_genai:gemini-2.0-flash-exp
# SCOPE_MODEL=google_genai:gemini-2.0-flash-exp

# ===== LOCAL MODEL PROVIDERS =====

# LM Studio (Local OpenAI-compatible API)
# Make sure LM Studio is running with API server enabled
# Supports model names with forward slashes
# REPORT_MODEL=lmstudio://localhost:1234/qwen/qwen3-4b-thinking-2507
# RESEARCH_MODEL=lmstudio://localhost:1234/microsoft/DialoGPT-medium
# SUMMARIZATION_MODEL=lmstudio://localhost:1234/microsoft/DialoGPT-medium
# COMPRESS_MODEL=lmstudio://localhost:1234/microsoft/DialoGPT-medium
# SUPERVISOR_MODEL=lmstudio://localhost:1234/microsoft/DialoGPT-medium
# SCOPE_MODEL=lmstudio://localhost:1234/microsoft/DialoGPT-medium

# Ollama (Local model server)
# Make sure Ollama is running: ollama serve
# REPORT_MODEL=ollama://localhost:11434/llama3.2:3b
# RESEARCH_MODEL=ollama://localhost:11434/llama3.2:3b
# SUMMARIZATION_MODEL=ollama://localhost:11434/llama3.2:3b
# COMPRESS_MODEL=ollama://localhost:11434/llama3.2:3b
# SUPERVISOR_MODEL=ollama://localhost:11434/llama3.2:3b
# SCOPE_MODEL=ollama://localhost:11434/llama3.2:3b

# Remote Ollama instance
# REPORT_MODEL=ollama://localhost:11434/gemma3-lc:12b
# RESEARCH_MODEL=ollama://localhost:11434/gemma3-lc:12b
# SUMMARIZATION_MODEL=ollama://localhost:11434/gemma3-lc:12b
# COMPRESS_MODEL=ollama://localhost:11434/gemma3-lc:12b
# SUPERVISOR_MODEL=ollama://localhost:11434/qwen3-lc:30b
# SCOPE_MODEL=ollama://localhost:11434/gemma3-lc:12b

# ===== MIXED CONFIGURATION EXAMPLE =====
# You can mix different providers for different purposes
# For example: Use cloud models for complex reasoning and local models for simple tasks

# High-performance cloud models for complex tasks
RESEARCH_MODEL=anthropic:claude-3-5-sonnet-20241022
SUPERVISOR_MODEL=anthropic:claude-3-5-sonnet-20241022
REPORT_MODEL=openai:gpt-4o

# Local models for simpler tasks
SUMMARIZATION_MODEL=ollama://localhost:11434/llama3.2:3b
COMPRESS_MODEL=lmstudio://localhost:1234/microsoft/DialoGPT-medium
SCOPE_MODEL=openai:gpt-4o-mini

# ===== API KEYS (Required for cloud providers) =====
# OpenAI
# OPENAI_API_KEY=your_openai_api_key_here

# Anthropic
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Generative AI
# GOOGLE_API_KEY=your_google_api_key_here

# ===== OTHER ENVIRONMENT VARIABLES =====
# Tavily Search API (for research functionality)
# TAVILY_API_KEY=your_tavily_api_key_here
